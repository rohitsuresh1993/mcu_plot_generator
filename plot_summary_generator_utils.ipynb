{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .py file with helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import wikipedia\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import chain\n",
    "from difflib import get_close_matches as gcm\n",
    "import string\n",
    "from imdb import IMDb\n",
    "import wikia\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract list of wiki page titles\n",
    "def titles_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    titles_list = movies['wikipedia page title']\n",
    "    return titles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract imdb movie ids\n",
    "def imdb_id_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    id_list = movies['imdb id']\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull the text of plot section from a movie wiki page\n",
    "def plot_puller_wiki(movie):\n",
    "    movie_plot = wikipedia.WikipediaPage(title = movie).section('Plot')\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_puller_imdb(imdb_movie_id):\n",
    "    ia = IMDb()\n",
    "    movie = ia.get_movie(imdb_movie_id)\n",
    "    movie_plot = str(movie['synopsis'])\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all movies to be inclued in the corpus\n",
    "def plot_aggregator_wiki(titles_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie in enumerate(titles_list):\n",
    "        movie_plot = plot_puller_wiki(movie)\n",
    "        plot_agg += movie_plot\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregator_imdb(imdb_movie_id_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie_id in enumerate(imdb_movie_id_list):\n",
    "        movie_plot = plot_puller_imdb(movie_id[2:])\n",
    "        plot_agg += movie_plot\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize words according to corresponding part of speech\n",
    "# will try to include different lemmatizers i.espacey, stanford, textblob\n",
    "def lemma(word):\n",
    "    if get_wordnet_pos(word) == 'r':\n",
    "        try:\n",
    "            possible_adj = []\n",
    "            for ss in wordnet.synsets(word):\n",
    "              for lemmas in ss.lemmas(): # all possible lemmas\n",
    "                  for ps in lemmas.pertainyms(): # all possible pertainyms\n",
    "                      possible_adj.append(ps.name())\n",
    "            word = gcm(word,possible_adj)[0]\n",
    "        except:\n",
    "            word = word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos = get_wordnet_pos(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "    return lemmatized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up and tokenize the raw text of the corpus\n",
    "def preprocess_corpus_text(raw_string,lemmatize=True):\n",
    "    raw_string = raw_string.lower()\n",
    "    transtable = str.maketrans('', '', string.punctuation)\n",
    "    raw_string = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', raw_string)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sentence_tokens = sent_tokenize(raw_string)\n",
    "    word_tokens  = []\n",
    "    for sentence in sentence_tokens:\n",
    "        clean_sentence = sentence.translate(transtable)\n",
    "        tok = word_tokenize(clean_sentence)\n",
    "        word_tokens.append(tok)\n",
    "    final_tokens = []\n",
    "    for sentence in word_tokens:\n",
    "        ntk = [w for w in sentence if not w in stop_words]\n",
    "        final_tokens.append(ntk)\n",
    "    if lemmatize:\n",
    "        lemmatized_tokens = []\n",
    "        for i in range(len(final_tokens)):\n",
    "            wordtoks = []\n",
    "            for word in final_tokens[i]:\n",
    "                wordlemma = lemma(word)\n",
    "                wordtoks.append(wordlemma)\n",
    "            lemmatized_tokens.append(wordtoks)\n",
    "        while [] in lemmatized_tokens:\n",
    "            lemmatized_tokens.remove([])\n",
    "        return lemmatized_tokens\n",
    "    else:\n",
    "        while [] in final_tokens:\n",
    "            final_tokens.remove([])\n",
    "        return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull plot synopsis from individual marvel wikia pages\n",
    "def comic_plot(comic_vol_issue, wiki = 'marvel'):\n",
    "    try:\n",
    "        #summary = wikia.summary(wiki, comic_vol_issue)\n",
    "        #summary = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', summary)\n",
    "        full_page = wikia.page(wiki, comic_vol_issue)\n",
    "        pg = full_page.content\n",
    "        pg = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', pg)\n",
    "        #key = summary[:key_length]\n",
    "        #start_index = pg.find(key)\n",
    "        #plot = pg[start_index:]\n",
    "        plot = pg.lower()\n",
    "        rm_list = ['featured characters:','supporting characters:','antagonists:', 'races and species:',\n",
    "                   'other characters:','locations:','items:','vehicles:','\\n','\\xa0']\n",
    "        plot = plot.replace('‘','\\'').replace('...','.').replace('•','.').replace(\"\\'\",\"\").replace('s.h.i.e.l.d.','SHIELD ').replace('’','\\'').replace('s. h. i. e. l. d. ','SHIELD ')\n",
    "        for item in rm_list:\n",
    "            plot = plot.replace(item,'')\n",
    "        if plot == '':\n",
    "            return None\n",
    "        else:\n",
    "            return plot\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find comic book titles in each 'all comics' page\n",
    "def comic_titles_finder(my_url):\n",
    "    uClient = uReq(my_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,'html.parser')\n",
    "    next_page_tag = page_soup.findAll('a',{'class':'category-page__pagination-next wds-button wds-is-secondary'})\n",
    "    if next_page_tag:\n",
    "        next_page_link = next_page_tag[0]['href']\n",
    "    else:\n",
    "        next_page_link = None\n",
    "    comic_titles = page_soup.findAll('a',{'class':'category-page__member-link'})\n",
    "    comic_vol_issue_list = []\n",
    "    for tag in comic_titles:\n",
    "        name = tag['title']\n",
    "        if 'Category:' not in name:\n",
    "            comic_vol_issue_list.append(name)\n",
    "    return comic_vol_issue_list, next_page_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to go through all 'all comics' pages and create csv containing full names of all comic books\n",
    "def make_comic_titles_list(page_url,target_file):\n",
    "    filename = target_file + '.csv'\n",
    "    f = open(filename,'w')\n",
    "    headers = 'comic_title\\n'\n",
    "    f.write(headers)\n",
    "    while page_url:\n",
    "        cl,page_url = comic_titles_finder(page_url)\n",
    "        for title in cl:\n",
    "            f.write(title.replace(',','')+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RUN ONLY ONCE'''\n",
    "#make_comic_titles_list(first_page_url,'comic_titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all comic books\n",
    "# manually delete target_file if it already exists\n",
    "def comic_plot_agg(titles_list,target_file):\n",
    "    filename = target_file + '.txt'\n",
    "    for i,title in enumerate(titles_list):\n",
    "        plot = comic_plot(title)\n",
    "        if plot:\n",
    "            with open(filename, 'a+') as f:\n",
    "                f.write(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pd.read_csv('comic_titles.csv',header=0,encoding = 'unicode_escape')['comic_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = cl[:1000]#['comic_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             All-New Wolverine Vol 1 1\n",
       "1                           Extraordinary X-Men Vol 1 2\n",
       "2                                    Hail Hydra Vol 1 4\n",
       "3                                         Thors Vol 1 4\n",
       "4                                X-Men Classic Vol 1 57\n",
       "5                                         Hulk Vol 3 10\n",
       "6                     Essential Series Vol 1 Avengers 8\n",
       "7                                         Hulk Vol 3 11\n",
       "8      X-Men Archives Featuring Captain Britain Vol 1 1\n",
       "9                      Essential Series Vol 1 Dazzler 1\n",
       "10                     1602 Witch Hunter Angela Vol 1 4\n",
       "11                            Age of Apocalypse Vol 2 5\n",
       "12                         Angela: Queen of Hel Vol 1 1\n",
       "13                  Captain America: Sam Wilson Vol 1 1\n",
       "14                  Captain America: Sam Wilson Vol 1 2\n",
       "15                       Captain America: White Vol 1 3\n",
       "16                                    Civil War Vol 2 5\n",
       "17                          Deadpool vs. Thanos Vol 1 3\n",
       "18                          Deadpool vs. Thanos Vol 1 4\n",
       "19                                        Groot Vol 1 5\n",
       "20            Howling Commandos of S.H.I.E.L.D. Vol 1 1\n",
       "21                          Invincible Iron Man Vol 3 2\n",
       "22                                       Karnak Vol 1 1\n",
       "23                                  Ms. Marvel Vol 3 19\n",
       "24                                 New Avengers Vol 4 1\n",
       "25                                 New Avengers Vol 4 2\n",
       "26                                S.H.I.E.L.D. Vol 3 11\n",
       "27                                        Siege Vol 2 4\n",
       "28                                Spider-Island Vol 1 5\n",
       "29                                   Weirdworld Vol 1 5\n",
       "                             ...                       \n",
       "970                            Complete Mystery Vol 1 4\n",
       "971                    Frankie and Lana Comics Vol 1 13\n",
       "972                                 Gay Comics Vol 1 36\n",
       "973                        Hedy De Vine Comics Vol 1 31\n",
       "974                              Justice Comics Vol 1 8\n",
       "975                                    Kid Colt Vol 1 4\n",
       "976                                        Lana Vol 1 4\n",
       "977                     Lawbreakers Always Lose Vol 1 6\n",
       "978                              Margie Comics Vol 1 45\n",
       "979                      Marvel Mystery Comics Vol 1 90\n",
       "980                           Millie the Model Vol 1 16\n",
       "981                      Miss America Magazine Vol 7 19\n",
       "982                          Mitzi's Boy Friend Vol 1 6\n",
       "983                           Nellie the Nurse Vol 1 17\n",
       "984                                Oscar Comics Vol 1 9\n",
       "985                         Sub-Mariner Comics Vol 1 30\n",
       "986                          Tessie the Typist Vol 1 20\n",
       "987                                  Tex Morgan Vol 1 4\n",
       "988                                 Two-Gun Kid Vol 1 6\n",
       "989                              Willie Comics Vol 1 18\n",
       "990                       All-True Crime Cases Vol 1 32\n",
       "991                                Blaze Carson Vol 1 4\n",
       "992                      Blonde Phantom Comics Vol 1 22\n",
       "993                     Captain America Comics Vol 1 71\n",
       "994                               Comedy Comics Vol 2 6\n",
       "995                               Crimefighters Vol 1 6\n",
       "996                    Georgie and Judy Comics Vol 1 21\n",
       "997                         Human Torch Comics Vol 1 35\n",
       "998                                       Ideal Vol 1 5\n",
       "999                                     Jeanie Vol 1 24\n",
       "Name: comic_title, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%time agg = comic_plot_agg(s,'comic_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time preprocess_corpus_text(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comic_corpus.txt') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3461181"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohit Suresh\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 7s\n"
     ]
    }
   ],
   "source": [
    "%time corpus_tokens = preprocess_corpus_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 49s\n"
     ]
    }
   ],
   "source": [
    "%time embed = Word2Vec(corpus_tokens, size = 300, min_count = 3, sg = 1, iter=500, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=7935, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['laura', 'allnew', 'wolverine', 'save', 'man', 'front', 'tower', 'assassinate', 'shot', 'head', 'fall', 'unconscious', 'remembers', 'conversation', 'logan', 'mission', 'fail', 'kill', 'drug', 'runner', 'attack', 'apologizes', 'explains', 'two', 'make']\n"
     ]
    }
   ],
   "source": [
    "embed_words = list(embed.wv.vocab)\n",
    "print(embed_words[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.84750974e-02  3.99098933e-01  5.17599918e-02 -7.15471685e-01\n",
      " -1.16666786e-01  2.57770121e-01  4.33020860e-01  1.36903417e+00\n",
      " -2.14118976e-03 -2.43924372e-02 -1.34073043e+00  1.77573133e-02\n",
      " -1.41533375e-01  7.59533839e-03 -7.30943918e-01 -2.62997281e-02\n",
      " -6.15421474e-01  5.05680330e-02 -4.43684489e-01  1.94002777e-01\n",
      " -5.40067494e-01  2.96885967e-01  1.76078603e-01  1.06869090e+00\n",
      "  3.10422957e-01 -7.59450734e-01  2.90578574e-01 -2.09600553e-01\n",
      " -3.03681821e-01  7.62366116e-01 -1.72329336e-01 -1.97839892e+00\n",
      "  2.10265428e-01  4.18375641e-01  5.12885273e-01 -2.76061535e-01\n",
      "  4.35416639e-01  3.97232175e-01 -1.27180064e+00  6.20522320e-01\n",
      " -1.80213705e-01  8.07522833e-01  2.96129793e-01  7.22569525e-01\n",
      " -2.50769090e-02  2.39775211e-01 -5.38984299e-01  2.40873218e-01\n",
      " -4.76988256e-01  9.41541135e-01 -3.64971198e-02 -1.47116458e+00\n",
      "  1.25569654e+00 -1.00636125e+00  1.32400370e+00  3.06404710e-01\n",
      " -6.53740108e-01  4.54577148e-01 -1.00918853e+00 -3.47118825e-01\n",
      "  1.56850845e-01  5.22304773e-02 -1.07633913e+00 -5.84290981e-01\n",
      " -2.24324539e-01  2.75034994e-01  1.01209033e+00 -6.05773747e-01\n",
      "  4.46714133e-01  3.28098200e-02 -2.81310648e-01  5.85707426e-01\n",
      "  1.49643853e-01  5.33193290e-01 -5.45112714e-02  1.21871317e-02\n",
      " -3.46249253e-01  2.48401657e-01 -1.21646774e+00 -1.06076114e-01\n",
      "  3.00915185e-02 -3.48568738e-01 -5.20138502e-01  2.55511343e-01\n",
      " -5.86382985e-01 -8.16910505e-01 -2.11404338e-01 -8.65125299e-01\n",
      "  3.70639086e-01 -7.99292684e-01  7.46784627e-01  1.52684852e-01\n",
      " -7.46324062e-01  7.41887331e-01  7.03015804e-01 -4.78694528e-01\n",
      "  1.38642526e+00  9.75975674e-03  2.18977392e-01  3.96883786e-01\n",
      "  4.68967974e-01 -3.30537260e-01  6.42792940e-01  3.92530560e-01\n",
      " -2.53616452e-01  2.14013457e-01  8.43170345e-01 -3.85557592e-01\n",
      "  6.36824369e-02 -5.46191752e-01 -1.70298859e-01 -1.10336661e+00\n",
      "  9.44817901e-01 -6.45722508e-01  3.68759662e-01  4.44277138e-01\n",
      "  2.76992500e-01  7.29734227e-02 -2.41342440e-01  4.43475813e-01\n",
      "  8.50873053e-01  1.84294462e-01  5.09343803e-01  7.96972513e-01\n",
      "  1.12377298e+00  1.99246824e-01  4.79752034e-01  5.20421445e-01\n",
      "  3.43932927e-01  2.87682354e-01  9.75522101e-01 -9.63073969e-01\n",
      " -1.98013827e-01 -6.46475852e-01  8.42561543e-01  9.73394513e-01\n",
      " -3.05970043e-01 -6.39638126e-01 -3.59930277e-01  8.10857177e-01\n",
      "  6.25979185e-01  3.73250954e-02 -4.92636591e-01 -7.17617631e-01\n",
      "  9.25310969e-01 -4.49080050e-01  1.36683717e-01  2.62074083e-01\n",
      " -8.02182019e-01  7.63030469e-01 -2.55888253e-01 -3.06253284e-01\n",
      " -7.55032063e-01 -6.22040570e-01 -2.15014040e-01  6.17884278e-01\n",
      "  4.90561485e-01 -2.31221378e-01 -4.86262381e-01 -5.10015368e-01\n",
      " -2.38292217e-01  2.17884481e-01  1.16427867e-02  3.79548401e-01\n",
      " -4.53349233e-01 -7.54373670e-01  5.97583771e-01 -6.65319145e-01\n",
      " -2.06510872e-01  3.69684368e-01  1.36837840e-01 -2.41751671e-01\n",
      " -6.46316484e-02  8.60865355e-01 -5.06115675e-01 -8.42374414e-02\n",
      "  5.88087970e-03 -1.56497031e-01 -3.70064169e-01 -4.67721552e-01\n",
      "  6.42248154e-01  7.36099958e-01  1.39045525e+00 -5.69225490e-01\n",
      "  7.43718982e-01 -2.80741334e-01  1.93974704e-01  1.85690999e-01\n",
      "  2.29786217e-01  1.46255597e-01  2.28746802e-01 -1.12021768e+00\n",
      " -7.74409533e-01  1.01234257e-01 -1.48660696e+00  5.57373106e-01\n",
      " -1.97464719e-01  1.64867654e-01 -3.84626389e-01  6.40122294e-01\n",
      " -1.53743118e-01  5.36233246e-01  1.59671567e-02  4.19232994e-01\n",
      " -7.47779965e-01  7.94004142e-01 -5.99132121e-01  2.68739283e-01\n",
      "  6.52479678e-02  3.81634951e-01 -3.48910809e-01 -8.49494517e-01\n",
      " -6.08176172e-01  1.36522233e+00 -1.68901384e-01  8.13383639e-01\n",
      " -4.09891248e-01 -7.54030883e-01 -1.47444513e-02  7.73351610e-01\n",
      " -2.83304155e-02 -4.75296348e-01  3.40550542e-01 -1.20834045e-01\n",
      "  4.62001413e-01 -2.81216562e-01 -5.15626252e-01 -1.23320222e-01\n",
      "  4.97581542e-01 -5.85409462e-01 -4.60197926e-01 -1.73051819e-01\n",
      "  1.48660481e-01 -7.36943066e-01  1.92592755e-01  4.39777598e-02\n",
      "  1.67338371e-01 -1.10674930e+00 -1.26531824e-01 -7.64739871e-01\n",
      "  3.43649566e-01 -4.02604967e-01 -8.89576495e-01 -1.57123893e-01\n",
      " -1.72297284e-02  9.25529361e-01  9.78362858e-02  1.37690222e+00\n",
      " -7.04206526e-01 -1.98788717e-01 -9.95002687e-01  1.79776216e+00\n",
      " -6.14713311e-01  3.69776845e-01  1.07091737e+00  5.28556705e-02\n",
      "  6.20014250e-01 -8.21471155e-01  6.83347046e-01  3.18638891e-01\n",
      " -1.69997334e-01  1.27323568e-01  1.00513196e+00  1.78275019e-01\n",
      "  1.13948569e-01  5.87756515e-01 -6.78282348e-04  6.84008896e-02\n",
      " -7.04682350e-01 -9.48826522e-02  9.14854944e-01  4.93570149e-01\n",
      "  9.79986966e-01  7.75540233e-01 -1.41417873e+00 -6.87098920e-01\n",
      " -6.58226371e-01  9.50400889e-01  7.30188489e-01  3.56224626e-01\n",
      " -1.18296959e-01 -6.50338888e-01  8.43626499e-01  2.40205482e-01\n",
      " -1.97534844e-01  1.41212372e-02  1.21834517e+00 -2.56752193e-01\n",
      "  1.41611055e-01 -7.03631699e-01 -5.96178353e-01  1.10602772e+00\n",
      "  1.35574058e-01 -7.68095315e-01  2.47484207e-01  5.30214667e-01\n",
      " -1.33068964e-01  2.01667115e-01  8.60507607e-01 -5.93835294e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embed.wv.__getitem__('thanos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = embed.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yellowjacket', 0.3419579565525055),\n",
       " ('ant', 0.2933281660079956),\n",
       " ('charlie27', 0.26632097363471985),\n",
       " ('handicap', 0.25764933228492737),\n",
       " ('sought', 0.2514813244342804),\n",
       " ('grat', 0.2513841688632965),\n",
       " ('lyander', 0.2510165274143219),\n",
       " ('piggy', 0.25082477927207947),\n",
       " ('weird', 0.2500923275947571),\n",
       " ('cahoot', 0.2467557191848755)]"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = word_vectors.most_similar(positive=['wasp','hank'],negative=['janet'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yellowjacket', 0.658576488494873),\n",
       " ('ant', 0.4543679356575012),\n",
       " ('spectrum', 0.4529518187046051),\n",
       " ('prism', 0.39774370193481445),\n",
       " ('chairman', 0.3977375626564026),\n",
       " ('charlie27', 0.39174193143844604),\n",
       " ('scarlet', 0.38669300079345703),\n",
       " ('arctic', 0.37767261266708374),\n",
       " ('korvac', 0.36474430561065674),\n",
       " ('yondu', 0.3645530939102173)]"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word('wasp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.84750974e-02  3.99098933e-01  5.17599918e-02 -7.15471685e-01\n",
      " -1.16666786e-01  2.57770121e-01  4.33020860e-01  1.36903417e+00\n",
      " -2.14118976e-03 -2.43924372e-02 -1.34073043e+00  1.77573133e-02\n",
      " -1.41533375e-01  7.59533839e-03 -7.30943918e-01 -2.62997281e-02\n",
      " -6.15421474e-01  5.05680330e-02 -4.43684489e-01  1.94002777e-01\n",
      " -5.40067494e-01  2.96885967e-01  1.76078603e-01  1.06869090e+00\n",
      "  3.10422957e-01 -7.59450734e-01  2.90578574e-01 -2.09600553e-01\n",
      " -3.03681821e-01  7.62366116e-01 -1.72329336e-01 -1.97839892e+00\n",
      "  2.10265428e-01  4.18375641e-01  5.12885273e-01 -2.76061535e-01\n",
      "  4.35416639e-01  3.97232175e-01 -1.27180064e+00  6.20522320e-01\n",
      " -1.80213705e-01  8.07522833e-01  2.96129793e-01  7.22569525e-01\n",
      " -2.50769090e-02  2.39775211e-01 -5.38984299e-01  2.40873218e-01\n",
      " -4.76988256e-01  9.41541135e-01 -3.64971198e-02 -1.47116458e+00\n",
      "  1.25569654e+00 -1.00636125e+00  1.32400370e+00  3.06404710e-01\n",
      " -6.53740108e-01  4.54577148e-01 -1.00918853e+00 -3.47118825e-01\n",
      "  1.56850845e-01  5.22304773e-02 -1.07633913e+00 -5.84290981e-01\n",
      " -2.24324539e-01  2.75034994e-01  1.01209033e+00 -6.05773747e-01\n",
      "  4.46714133e-01  3.28098200e-02 -2.81310648e-01  5.85707426e-01\n",
      "  1.49643853e-01  5.33193290e-01 -5.45112714e-02  1.21871317e-02\n",
      " -3.46249253e-01  2.48401657e-01 -1.21646774e+00 -1.06076114e-01\n",
      "  3.00915185e-02 -3.48568738e-01 -5.20138502e-01  2.55511343e-01\n",
      " -5.86382985e-01 -8.16910505e-01 -2.11404338e-01 -8.65125299e-01\n",
      "  3.70639086e-01 -7.99292684e-01  7.46784627e-01  1.52684852e-01\n",
      " -7.46324062e-01  7.41887331e-01  7.03015804e-01 -4.78694528e-01\n",
      "  1.38642526e+00  9.75975674e-03  2.18977392e-01  3.96883786e-01\n",
      "  4.68967974e-01 -3.30537260e-01  6.42792940e-01  3.92530560e-01\n",
      " -2.53616452e-01  2.14013457e-01  8.43170345e-01 -3.85557592e-01\n",
      "  6.36824369e-02 -5.46191752e-01 -1.70298859e-01 -1.10336661e+00\n",
      "  9.44817901e-01 -6.45722508e-01  3.68759662e-01  4.44277138e-01\n",
      "  2.76992500e-01  7.29734227e-02 -2.41342440e-01  4.43475813e-01\n",
      "  8.50873053e-01  1.84294462e-01  5.09343803e-01  7.96972513e-01\n",
      "  1.12377298e+00  1.99246824e-01  4.79752034e-01  5.20421445e-01\n",
      "  3.43932927e-01  2.87682354e-01  9.75522101e-01 -9.63073969e-01\n",
      " -1.98013827e-01 -6.46475852e-01  8.42561543e-01  9.73394513e-01\n",
      " -3.05970043e-01 -6.39638126e-01 -3.59930277e-01  8.10857177e-01\n",
      "  6.25979185e-01  3.73250954e-02 -4.92636591e-01 -7.17617631e-01\n",
      "  9.25310969e-01 -4.49080050e-01  1.36683717e-01  2.62074083e-01\n",
      " -8.02182019e-01  7.63030469e-01 -2.55888253e-01 -3.06253284e-01\n",
      " -7.55032063e-01 -6.22040570e-01 -2.15014040e-01  6.17884278e-01\n",
      "  4.90561485e-01 -2.31221378e-01 -4.86262381e-01 -5.10015368e-01\n",
      " -2.38292217e-01  2.17884481e-01  1.16427867e-02  3.79548401e-01\n",
      " -4.53349233e-01 -7.54373670e-01  5.97583771e-01 -6.65319145e-01\n",
      " -2.06510872e-01  3.69684368e-01  1.36837840e-01 -2.41751671e-01\n",
      " -6.46316484e-02  8.60865355e-01 -5.06115675e-01 -8.42374414e-02\n",
      "  5.88087970e-03 -1.56497031e-01 -3.70064169e-01 -4.67721552e-01\n",
      "  6.42248154e-01  7.36099958e-01  1.39045525e+00 -5.69225490e-01\n",
      "  7.43718982e-01 -2.80741334e-01  1.93974704e-01  1.85690999e-01\n",
      "  2.29786217e-01  1.46255597e-01  2.28746802e-01 -1.12021768e+00\n",
      " -7.74409533e-01  1.01234257e-01 -1.48660696e+00  5.57373106e-01\n",
      " -1.97464719e-01  1.64867654e-01 -3.84626389e-01  6.40122294e-01\n",
      " -1.53743118e-01  5.36233246e-01  1.59671567e-02  4.19232994e-01\n",
      " -7.47779965e-01  7.94004142e-01 -5.99132121e-01  2.68739283e-01\n",
      "  6.52479678e-02  3.81634951e-01 -3.48910809e-01 -8.49494517e-01\n",
      " -6.08176172e-01  1.36522233e+00 -1.68901384e-01  8.13383639e-01\n",
      " -4.09891248e-01 -7.54030883e-01 -1.47444513e-02  7.73351610e-01\n",
      " -2.83304155e-02 -4.75296348e-01  3.40550542e-01 -1.20834045e-01\n",
      "  4.62001413e-01 -2.81216562e-01 -5.15626252e-01 -1.23320222e-01\n",
      "  4.97581542e-01 -5.85409462e-01 -4.60197926e-01 -1.73051819e-01\n",
      "  1.48660481e-01 -7.36943066e-01  1.92592755e-01  4.39777598e-02\n",
      "  1.67338371e-01 -1.10674930e+00 -1.26531824e-01 -7.64739871e-01\n",
      "  3.43649566e-01 -4.02604967e-01 -8.89576495e-01 -1.57123893e-01\n",
      " -1.72297284e-02  9.25529361e-01  9.78362858e-02  1.37690222e+00\n",
      " -7.04206526e-01 -1.98788717e-01 -9.95002687e-01  1.79776216e+00\n",
      " -6.14713311e-01  3.69776845e-01  1.07091737e+00  5.28556705e-02\n",
      "  6.20014250e-01 -8.21471155e-01  6.83347046e-01  3.18638891e-01\n",
      " -1.69997334e-01  1.27323568e-01  1.00513196e+00  1.78275019e-01\n",
      "  1.13948569e-01  5.87756515e-01 -6.78282348e-04  6.84008896e-02\n",
      " -7.04682350e-01 -9.48826522e-02  9.14854944e-01  4.93570149e-01\n",
      "  9.79986966e-01  7.75540233e-01 -1.41417873e+00 -6.87098920e-01\n",
      " -6.58226371e-01  9.50400889e-01  7.30188489e-01  3.56224626e-01\n",
      " -1.18296959e-01 -6.50338888e-01  8.43626499e-01  2.40205482e-01\n",
      " -1.97534844e-01  1.41212372e-02  1.21834517e+00 -2.56752193e-01\n",
      "  1.41611055e-01 -7.03631699e-01 -5.96178353e-01  1.10602772e+00\n",
      "  1.35574058e-01 -7.68095315e-01  2.47484207e-01  5.30214667e-01\n",
      " -1.33068964e-01  2.01667115e-01  8.60507607e-01 -5.93835294e-01]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.__getitem__('thanos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
