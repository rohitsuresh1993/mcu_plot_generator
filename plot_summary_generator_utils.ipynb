{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .py file with helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import wikipedia\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from imdb import IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract list of wiki page titles\n",
    "def titles_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    titles_list = movies['wikipedia page title']\n",
    "    return titles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract imdb movie ids\n",
    "def imdb_id_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    id_list = movies['imdb id']\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull the text of plot section from a movie wiki page\n",
    "def plot_puller_wiki(movie):\n",
    "    movie_plot = wikipedia.WikipediaPage(title = movie).section('Plot')\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_puller_imdb(imdb_movie_id):\n",
    "    ia = IMDb()\n",
    "    movie = ia.get_movie(imdb_movie_id)\n",
    "    movie_plot = str(movie['synopsis'])\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all movies to be inclued in the corpus\n",
    "def plot_aggregator_wiki(titles_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie in enumerate(titles_list):\n",
    "        movie_plot = plot_puller_wiki(movie)\n",
    "        plot_agg += movie_plot\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregator_imdb(imdb_movie_id_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie_id in enumerate(imdb_movie_id_list):\n",
    "        movie_plot = plot_puller_imdb(movie_id[2:])\n",
    "        plot_agg += movie_plot\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up and tokenize the raw text of the corpus\n",
    "def preprocess_corpus_text(raw_string,lemmatize=True):\n",
    "    transtable = str.maketrans('', '', string.punctuation)\n",
    "    raw_string = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', raw_string)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sentence_tokens = sent_tokenize(raw_string)\n",
    "    word_tokens  = []\n",
    "    for sentence in sentence_tokens:\n",
    "        clean_sentence = sentence.translate(transtable)\n",
    "        tok = word_tokenize(clean_sentence)\n",
    "        word_tokens.append(tok)\n",
    "    final_tokens = []\n",
    "    for sentence in word_tokens:\n",
    "        ntk = [w for w in sentence if not w in stop_words]\n",
    "        final_tokens.append(ntk)\n",
    "    if lemmatize:\n",
    "        lemmatized_tokens = []\n",
    "        for i in range(len(final_tokens)):\n",
    "            wordtoks = []\n",
    "            for word in final_tokens[i]:\n",
    "                wordlemma = wordnet_lemmatizer.lemmatize(word,pos='v')\n",
    "                wordtoks.append(wordlemma)\n",
    "            lemmatized_tokens.append(wordtoks)\n",
    "        return lemmatized_tokens\n",
    "    else:\n",
    "        return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teststr = 'the next day steve is summoned to the brooklyn bunker to see phillips and stark. steve is approached by a beautiful female officer who wishes to thank him for his service the best way she knows how. peggy walks in on steve kissing the enlisted-woman and angrily storms away. steve apologetically follows her to starks lab, insisting that he gets nervous around women and asks why he should apologize if carter and stark have a thing going. stark quickly shoots down the rumored relationship and takes steve to his weapons engineering lab. he remarks that rogers has become attached to the triangular shield, which steve says is a handy tool in the field. on a table are several prototype shields with sophisticated components, however steve finds a plain, circular shield on a lower shelf.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_corpus_text(teststr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
