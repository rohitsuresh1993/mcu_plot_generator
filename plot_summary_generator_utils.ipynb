{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .py file with helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import wikipedia\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import chain\n",
    "from difflib import get_close_matches as gcm\n",
    "import string\n",
    "from imdb import IMDb\n",
    "import wikia\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract list of wiki page titles\n",
    "def titles_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    titles_list = movies['wikipedia page title']\n",
    "    return titles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract imdb movie ids\n",
    "def imdb_id_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    id_list = movies['imdb id']\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull the text of plot section from a movie wiki page\n",
    "def plot_puller_wiki(movie):\n",
    "    movie_plot = wikipedia.WikipediaPage(title = movie).section('Plot')\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_puller_imdb(imdb_movie_id):\n",
    "    ia = IMDb()\n",
    "    movie = ia.get_movie(imdb_movie_id)\n",
    "    movie_plot = str(movie['synopsis'])\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all movies to be inclued in the corpus\n",
    "def plot_aggregator_wiki(titles_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie in enumerate(titles_list):\n",
    "        movie_plot = plot_puller_wiki(movie)\n",
    "        plot_agg += movie_plot\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregator_imdb(imdb_movie_id_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie_id in enumerate(imdb_movie_id_list):\n",
    "        movie_plot = plot_puller_imdb(movie_id[2:])\n",
    "        plot_agg += movie_plot\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize words according to corresponding part of speech\n",
    "# will try to include different lemmatizers i.espacey, stanford, textblob\n",
    "def lemma(word):\n",
    "    if get_wordnet_pos(word) == 'r':\n",
    "        try:\n",
    "            possible_adj = []\n",
    "            for ss in wordnet.synsets(word):\n",
    "              for lemmas in ss.lemmas(): # all possible lemmas\n",
    "                  for ps in lemmas.pertainyms(): # all possible pertainyms\n",
    "                      possible_adj.append(ps.name())\n",
    "            word = gcm(word,possible_adj)[0]\n",
    "        except:\n",
    "            word = word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos = get_wordnet_pos(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "    return lemmatized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up and tokenize the raw text of the corpus\n",
    "def preprocess_corpus_text(raw_string,lemmatize=True):\n",
    "    raw_string = raw_string.lower()\n",
    "    transtable = str.maketrans('', '', string.punctuation)\n",
    "    raw_string = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', raw_string)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sentence_tokens = sent_tokenize(raw_string)\n",
    "    word_tokens  = []\n",
    "    for sentence in sentence_tokens:\n",
    "        clean_sentence = sentence.translate(transtable)\n",
    "        tok = word_tokenize(clean_sentence)\n",
    "        word_tokens.append(tok)\n",
    "    final_tokens = []\n",
    "    for sentence in word_tokens:\n",
    "        ntk = [w for w in sentence if not w in stop_words]\n",
    "        final_tokens.append(ntk)\n",
    "    if lemmatize:\n",
    "        lemmatized_tokens = []\n",
    "        for i in range(len(final_tokens)):\n",
    "            wordtoks = []\n",
    "            for word in final_tokens[i]:\n",
    "                wordlemma = lemma(word)\n",
    "                wordtoks.append(wordlemma)\n",
    "            lemmatized_tokens.append(wordtoks)\n",
    "        while [] in lemmatized_tokens:\n",
    "            lemmatized_tokens.remove([])\n",
    "        return lemmatized_tokens\n",
    "    else:\n",
    "        while [] in final_tokens:\n",
    "            final_tokens.remove([])\n",
    "        return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull plot synopsis from individual marvel wikia pages\n",
    "def comic_plot(comic_vol_issue, wiki = 'marvel'):\n",
    "    try:\n",
    "        #summary = wikia.summary(wiki, comic_vol_issue)\n",
    "        #summary = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', summary)\n",
    "        full_page = wikia.page(wiki, comic_vol_issue)\n",
    "        pg = full_page.content\n",
    "        pg = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', pg)\n",
    "        #key = summary[:key_length]\n",
    "        #start_index = pg.find(key)\n",
    "        #plot = pg[start_index:]\n",
    "        plot = pg.lower()\n",
    "        rm_list = ['featured characters:','supporting characters:','antagonists:', 'races and species:',\n",
    "                   'other characters:','locations:','items:','vehicles:','villains:','\\n','\\xa0']\n",
    "        plot = plot.replace('‘','\\'').replace('...','.').replace('…','.').replace('•','.').replace(\"\\'\",\"\").replace('s.h.i.e.l.d.','SHIELD ').replace('’','\\'').replace('s. h. i. e. l. d. ','SHIELD ')\n",
    "        for item in rm_list:\n",
    "            plot = plot.replace(item,'')\n",
    "        if plot == '':\n",
    "            return None\n",
    "        else:\n",
    "            return plot\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find comic book titles in each 'all comics' page\n",
    "def comic_titles_finder(my_url):\n",
    "    uClient = uReq(my_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,'html.parser')\n",
    "    next_page_tag = page_soup.findAll('a',{'class':'category-page__pagination-next wds-button wds-is-secondary'})\n",
    "    if next_page_tag:\n",
    "        next_page_link = next_page_tag[0]['href']\n",
    "    else:\n",
    "        next_page_link = None\n",
    "    comic_titles = page_soup.findAll('a',{'class':'category-page__member-link'})\n",
    "    comic_vol_issue_list = []\n",
    "    for tag in comic_titles:\n",
    "        name = tag['title']\n",
    "        if 'Category:' not in name:\n",
    "            comic_vol_issue_list.append(name)\n",
    "    return comic_vol_issue_list, next_page_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to go through all 'all comics' pages and create csv containing full names of all comic books\n",
    "def make_comic_titles_list(page_url,target_file):\n",
    "    filename = target_file + '.csv'\n",
    "    f = open(filename,'w')\n",
    "    headers = 'comic_title\\n'\n",
    "    f.write(headers)\n",
    "    while page_url:\n",
    "        cl,page_url = comic_titles_finder(page_url)\n",
    "        for title in cl:\n",
    "            f.write(title.replace(',','')+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN ONLY ONCE\n",
    "#make_comic_titles_list(first_page_url,'comic_titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all comic books\n",
    "# manually delete target_file if it already exists\n",
    "def comic_plot_agg(titles_list,target_file):\n",
    "    filename = target_file + '.txt'\n",
    "    for i,title in enumerate(titles_list):\n",
    "        plot = comic_plot(title)\n",
    "        if plot:\n",
    "            with open(filename, 'a+', encoding=\"utf-8\") as f:\n",
    "                f.write(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_ids():\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
