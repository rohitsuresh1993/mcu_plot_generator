{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .py file with helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import wikipedia\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import chain\n",
    "from difflib import get_close_matches as gcm\n",
    "import string\n",
    "from imdb import IMDb\n",
    "import wikia\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract list of wiki page titles\n",
    "def titles_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    titles_list = movies['wikipedia page title']\n",
    "    return titles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract imdb movie ids\n",
    "def imdb_id_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    id_list = movies['imdb id']\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull the text of plot section from a movie wiki page\n",
    "def plot_puller_wiki(movie):\n",
    "    movie_plot = wikipedia.WikipediaPage(title = movie).section('Plot')\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_puller_imdb(imdb_movie_id):\n",
    "    ia = IMDb()\n",
    "    movie = ia.get_movie(imdb_movie_id)\n",
    "    movie_plot = str(movie['synopsis'])\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all movies to be inclued in the corpus\n",
    "def plot_aggregator_wiki(titles_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie in enumerate(titles_list):\n",
    "        movie_plot = plot_puller_wiki(movie)\n",
    "        plot_agg += movie_plot\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregator_imdb(imdb_movie_id_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie_id in enumerate(imdb_movie_id_list):\n",
    "        movie_plot = plot_puller_imdb(movie_id[2:])\n",
    "        plot_agg += movie_plot\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize words according to corresponding part of speech\n",
    "# will try to include different lemmatizers i.espacey, stanford, textblob\n",
    "def lemma(word):\n",
    "    if get_wordnet_pos(word) == 'r':\n",
    "        try:\n",
    "            possible_adj = []\n",
    "            for ss in wordnet.synsets(word):\n",
    "              for lemmas in ss.lemmas(): # all possible lemmas\n",
    "                  for ps in lemmas.pertainyms(): # all possible pertainyms\n",
    "                      possible_adj.append(ps.name())\n",
    "            word = gcm(word,possible_adj)[0]\n",
    "        except:\n",
    "            word = word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos = get_wordnet_pos(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "    return lemmatized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up and tokenize the raw text of the corpus\n",
    "def preprocess_corpus_text(raw_string,lemmatize=True):\n",
    "    raw_string = raw_string.lower()\n",
    "    transtable = str.maketrans('', '', string.punctuation)\n",
    "    raw_string = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', raw_string)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sentence_tokens = sent_tokenize(raw_string)\n",
    "    word_tokens  = []\n",
    "    for sentence in sentence_tokens:\n",
    "        clean_sentence = sentence.translate(transtable)\n",
    "        tok = word_tokenize(clean_sentence)\n",
    "        word_tokens.append(tok)\n",
    "    final_tokens = []\n",
    "    for sentence in word_tokens:\n",
    "        ntk = [w for w in sentence if not w in stop_words]\n",
    "        final_tokens.append(ntk)\n",
    "    if lemmatize:\n",
    "        lemmatized_tokens = []\n",
    "        for i in range(len(final_tokens)):\n",
    "            wordtoks = []\n",
    "            for word in final_tokens[i]:\n",
    "                wordlemma = lemma(word)\n",
    "                wordtoks.append(wordlemma)\n",
    "            lemmatized_tokens.append(wordtoks)\n",
    "        while [] in lemmatized_tokens:\n",
    "            lemmatized_tokens.remove([])\n",
    "        return lemmatized_tokens\n",
    "    else:\n",
    "        while [] in final_tokens:\n",
    "            final_tokens.remove([])\n",
    "        return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull plot synopsis from individual marvel wikia pages\n",
    "def comic_plot(comic_vol_issue, wiki = 'marvel'):\n",
    "    try:\n",
    "        #summary = wikia.summary(wiki, comic_vol_issue)\n",
    "        #summary = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', summary)\n",
    "        full_page = wikia.page(wiki, comic_vol_issue)\n",
    "        pg = full_page.content\n",
    "        pg = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', pg)\n",
    "        #key = summary[:key_length]\n",
    "        #start_index = pg.find(key)\n",
    "        #plot = pg[start_index:]\n",
    "        plot = pg.lower()\n",
    "        rm_list = ['featured characters:','supporting characters:','antagonists:', 'races and species:',\n",
    "                   'other characters:','locations:','items:','vehicles:','\\n','\\xa0']\n",
    "        plot = plot.replace('‘','\\'').replace('...','.').replace('•','.').replace(\"\\'\",\"\").replace('s.h.i.e.l.d.','SHIELD ').replace('’','\\'').replace('s. h. i. e. l. d. ','SHIELD ')\n",
    "        for item in rm_list:\n",
    "            plot = plot.replace(item,'')\n",
    "        if plot == '':\n",
    "            return None\n",
    "        else:\n",
    "            return plot\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find comic book titles in each 'all comics' page\n",
    "def comic_titles_finder(my_url):\n",
    "    uClient = uReq(my_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,'html.parser')\n",
    "    next_page_tag = page_soup.findAll('a',{'class':'category-page__pagination-next wds-button wds-is-secondary'})\n",
    "    if next_page_tag:\n",
    "        next_page_link = next_page_tag[0]['href']\n",
    "    else:\n",
    "        next_page_link = None\n",
    "    comic_titles = page_soup.findAll('a',{'class':'category-page__member-link'})\n",
    "    comic_vol_issue_list = []\n",
    "    for tag in comic_titles:\n",
    "        name = tag['title']\n",
    "        if 'Category:' not in name:\n",
    "            comic_vol_issue_list.append(name)\n",
    "    return comic_vol_issue_list, next_page_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to go through all 'all comics' pages and create csv containing full names of all comic books\n",
    "def make_comic_titles_list(page_url,target_file):\n",
    "    filename = target_file + '.csv'\n",
    "    f = open(filename,'w')\n",
    "    headers = 'comic_title\\n'\n",
    "    f.write(headers)\n",
    "    while page_url:\n",
    "        cl,page_url = comic_titles_finder(page_url)\n",
    "        for title in cl:\n",
    "            f.write(title.replace(',','')+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RUN ONLY ONCE'''\n",
    "#make_comic_titles_list(first_page_url,'comic_titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all comic books\n",
    "def comic_plot_agg(titles_list,target_file):\n",
    "    filename = target_file + '.txt'\n",
    "    for i,title in enumerate(titles_list):\n",
    "        plot = comic_plot(title)\n",
    "        if plot:\n",
    "            with open(filename, 'a+') as f:\n",
    "                f.write(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pd.read_csv('comic_titles.csv',header=0,encoding = 'unicode_escape')['comic_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = cl[:1000]#['comic_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             All-New Wolverine Vol 1 1\n",
       "1                           Extraordinary X-Men Vol 1 2\n",
       "2                                    Hail Hydra Vol 1 4\n",
       "3                                         Thors Vol 1 4\n",
       "4                                X-Men Classic Vol 1 57\n",
       "5                                         Hulk Vol 3 10\n",
       "6                     Essential Series Vol 1 Avengers 8\n",
       "7                                         Hulk Vol 3 11\n",
       "8      X-Men Archives Featuring Captain Britain Vol 1 1\n",
       "9                      Essential Series Vol 1 Dazzler 1\n",
       "10                     1602 Witch Hunter Angela Vol 1 4\n",
       "11                            Age of Apocalypse Vol 2 5\n",
       "12                         Angela: Queen of Hel Vol 1 1\n",
       "13                  Captain America: Sam Wilson Vol 1 1\n",
       "14                  Captain America: Sam Wilson Vol 1 2\n",
       "15                       Captain America: White Vol 1 3\n",
       "16                                    Civil War Vol 2 5\n",
       "17                          Deadpool vs. Thanos Vol 1 3\n",
       "18                          Deadpool vs. Thanos Vol 1 4\n",
       "19                                        Groot Vol 1 5\n",
       "20            Howling Commandos of S.H.I.E.L.D. Vol 1 1\n",
       "21                          Invincible Iron Man Vol 3 2\n",
       "22                                       Karnak Vol 1 1\n",
       "23                                  Ms. Marvel Vol 3 19\n",
       "24                                 New Avengers Vol 4 1\n",
       "25                                 New Avengers Vol 4 2\n",
       "26                                S.H.I.E.L.D. Vol 3 11\n",
       "27                                        Siege Vol 2 4\n",
       "28                                Spider-Island Vol 1 5\n",
       "29                                   Weirdworld Vol 1 5\n",
       "                             ...                       \n",
       "970                            Complete Mystery Vol 1 4\n",
       "971                    Frankie and Lana Comics Vol 1 13\n",
       "972                                 Gay Comics Vol 1 36\n",
       "973                        Hedy De Vine Comics Vol 1 31\n",
       "974                              Justice Comics Vol 1 8\n",
       "975                                    Kid Colt Vol 1 4\n",
       "976                                        Lana Vol 1 4\n",
       "977                     Lawbreakers Always Lose Vol 1 6\n",
       "978                              Margie Comics Vol 1 45\n",
       "979                      Marvel Mystery Comics Vol 1 90\n",
       "980                           Millie the Model Vol 1 16\n",
       "981                      Miss America Magazine Vol 7 19\n",
       "982                          Mitzi's Boy Friend Vol 1 6\n",
       "983                           Nellie the Nurse Vol 1 17\n",
       "984                                Oscar Comics Vol 1 9\n",
       "985                         Sub-Mariner Comics Vol 1 30\n",
       "986                          Tessie the Typist Vol 1 20\n",
       "987                                  Tex Morgan Vol 1 4\n",
       "988                                 Two-Gun Kid Vol 1 6\n",
       "989                              Willie Comics Vol 1 18\n",
       "990                       All-True Crime Cases Vol 1 32\n",
       "991                                Blaze Carson Vol 1 4\n",
       "992                      Blonde Phantom Comics Vol 1 22\n",
       "993                     Captain America Comics Vol 1 71\n",
       "994                               Comedy Comics Vol 2 6\n",
       "995                               Crimefighters Vol 1 6\n",
       "996                    Georgie and Judy Comics Vol 1 21\n",
       "997                         Human Torch Comics Vol 1 35\n",
       "998                                       Ideal Vol 1 5\n",
       "999                                     Jeanie Vol 1 24\n",
       "Name: comic_title, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%time agg = comic_plot_agg(s,'comic_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time preprocess_corpus_text(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
