{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .py file with helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohit Suresh\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import wikipedia\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import chain\n",
    "from difflib import get_close_matches as gcm\n",
    "import string\n",
    "from imdb import IMDb\n",
    "import wikia\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from random import sample\n",
    "import gensim\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract list of wiki page titles\n",
    "def titles_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    titles_list = movies['wikipedia page title']\n",
    "    return titles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract imdb movie ids\n",
    "def imdb_id_list(filepath):\n",
    "    movies = pd.read_excel(open(filepath,'rb'))\n",
    "    id_list = movies['imdb id']\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull the text of plot section from a movie wiki page\n",
    "def plot_puller_wiki(movie):\n",
    "    movie_plot = wikipedia.WikipediaPage(title = movie).section('Plot')\n",
    "    movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "    return movie_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_puller_imdb(imdb_movie_id):\n",
    "    ia = IMDb()\n",
    "    movie = ia.get_movie(imdb_movie_id)\n",
    "    try:\n",
    "        movie_plot = str(movie['synopsis'])\n",
    "        movie_plot = movie_plot.replace('\\n','').replace(\"\\'\",\"\").replace(\"\\\\\",\"\").lower()\n",
    "        return movie_plot\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all movies to be inclued in the corpus\n",
    "def plot_aggregator_wiki(titles_list):\n",
    "    plot_agg = ''\n",
    "    for i, movie in enumerate(titles_list):\n",
    "        movie_plot = plot_puller_wiki(movie)\n",
    "        if movie_plot:\n",
    "            plot_agg += movie_plot\n",
    "        else:\n",
    "            plot_agg = plot_agg\n",
    "    return plot_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregator_imdb(imdb_movie_id_list, target_file):\n",
    "    filename = target_file+'.txt'\n",
    "    plot_agg = ''\n",
    "    for i, movie_id in enumerate(imdb_movie_id_list):\n",
    "        movie_plot = plot_puller_imdb(movie_id[2:])\n",
    "        if movie_plot:\n",
    "            with open(filename, 'a+', encoding=\"utf-8\") as f:\n",
    "                f.write(movie_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmatize words according to corresponding part of speech\n",
    "# will try to include different lemmatizers i.espacey, stanford, textblob\n",
    "def lemma(word):\n",
    "    if get_wordnet_pos(word) == 'r':\n",
    "        try:\n",
    "            possible_adj = []\n",
    "            for ss in wordnet.synsets(word):\n",
    "              for lemmas in ss.lemmas(): # all possible lemmas\n",
    "                  for ps in lemmas.pertainyms(): # all possible pertainyms\n",
    "                      possible_adj.append(ps.name())\n",
    "            word = gcm(word,possible_adj)[0]\n",
    "        except:\n",
    "            word = word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos = get_wordnet_pos(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "    return lemmatized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up and tokenize the raw text of the corpus\n",
    "def preprocess_corpus_text(raw_string,lemmatize = True,remove_stopwords = True):\n",
    "    raw_string = raw_string.lower()\n",
    "    transtable = str.maketrans('', '', string.punctuation)\n",
    "    raw_string = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', raw_string)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sentence_tokens = sent_tokenize(raw_string)\n",
    "    word_tokens  = []\n",
    "    for sentence in sentence_tokens:\n",
    "        clean_sentence = sentence.translate(transtable)\n",
    "        tok = word_tokenize(clean_sentence)\n",
    "        word_tokens.append(tok)\n",
    "    final_tokens = []\n",
    "    if remove_stopwords:\n",
    "        for sentence in word_tokens:\n",
    "            ntk = [w for w in sentence if not w in stop_words]\n",
    "            final_tokens.append(ntk)\n",
    "    else:\n",
    "        final_tokens = word_tokens\n",
    "    if lemmatize:\n",
    "        lemmatized_tokens = []\n",
    "        for i in range(len(final_tokens)):\n",
    "            wordtoks = []\n",
    "            for word in final_tokens[i]:\n",
    "                wordlemma = lemma(word)\n",
    "                wordtoks.append(wordlemma)\n",
    "            lemmatized_tokens.append(wordtoks)\n",
    "        while [] in lemmatized_tokens:\n",
    "            lemmatized_tokens.remove([])\n",
    "        return lemmatized_tokens\n",
    "    else:\n",
    "        while [] in final_tokens:\n",
    "            final_tokens.remove([])\n",
    "        return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull plot synopsis from individual marvel wikia pages\n",
    "def comic_plot(comic_vol_issue, wiki = 'marvel'):\n",
    "    try:\n",
    "        #summary = wikia.summary(wiki, comic_vol_issue)\n",
    "        #summary = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', summary)\n",
    "        full_page = wikia.page(wiki, comic_vol_issue)\n",
    "        pg = full_page.content\n",
    "        pg = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', pg)\n",
    "        #key = summary[:key_length]\n",
    "        #start_index = pg.find(key)\n",
    "        #plot = pg[start_index:]\n",
    "        plot = pg.lower()\n",
    "        rm_list = ['featured characters:','supporting characters:','antagonists:', 'races and species:',\n",
    "                   'other characters:','locations:','items:','vehicles:','villains:','\\n','\\xa0']\n",
    "        plot = plot.replace('‘','\\'').replace('...','.').replace('…','.').replace('•','.').replace(\"\\'\",\"\").replace('s.h.i.e.l.d.','SHIELD ').replace('’','\\'').replace('s. h. i. e. l. d. ','SHIELD ')\n",
    "        for item in rm_list:\n",
    "            plot = plot.replace(item,'')\n",
    "        if plot == '':\n",
    "            return None\n",
    "        else:\n",
    "            return plot\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find comic book titles in each 'all comics' page\n",
    "def comic_titles_finder(my_url):\n",
    "    uClient = uReq(my_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = soup(page_html,'html.parser')\n",
    "    next_page_tag = page_soup.findAll('a',{'class':'category-page__pagination-next wds-button wds-is-secondary'})\n",
    "    if next_page_tag:\n",
    "        next_page_link = next_page_tag[0]['href']\n",
    "    else:\n",
    "        next_page_link = None\n",
    "    comic_titles = page_soup.findAll('a',{'class':'category-page__member-link'})\n",
    "    comic_vol_issue_list = []\n",
    "    for tag in comic_titles:\n",
    "        name = tag['title']\n",
    "        if 'Category:' not in name:\n",
    "            comic_vol_issue_list.append(name)\n",
    "    return comic_vol_issue_list, next_page_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to go through all 'all comics' pages and create csv containing full names of all comic books\n",
    "def make_comic_titles_list(page_url,target_file):\n",
    "    filename = target_file + '.csv'\n",
    "    f = open(filename,'w')\n",
    "    headers = 'comic_title\\n'\n",
    "    f.write(headers)\n",
    "    while page_url:\n",
    "        cl,page_url = comic_titles_finder(page_url)\n",
    "        for title in cl:\n",
    "            f.write(title.replace(',','')+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN ONLY ONCE\n",
    "#make_comic_titles_list(first_page_url,'comic_titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to aggregate plots of all comic books\n",
    "# manually delete target_file if it already exists\n",
    "def comic_plot_agg(titles_list,target_file):\n",
    "    filename = target_file + '.txt'\n",
    "    for i,title in enumerate(titles_list):\n",
    "        plot = comic_plot(title)\n",
    "        if plot:\n",
    "            with open(filename, 'a+', encoding=\"utf-8\") as f:\n",
    "                f.write(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(embedding):\n",
    "    vocab = embedding.wv.vocab\n",
    "    all_words = list(vocab)\n",
    "    vocab_size = len(all_words) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size,embedding.vector_size))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for i in range(len(all_words)):\n",
    "        weight_matrix[i + 1] = embedding[all_words[i]]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
